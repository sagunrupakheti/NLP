{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks and Language Models\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for assignment this week\n",
    "\n",
    "# ------------------\n",
    "# like github copilot \n",
    "# autocomplete\n",
    "\n",
    "# editor\n",
    "# import numpy as np\n",
    "#              as npy\n",
    "\n",
    "\n",
    "# language modeling -> generate jokes, stories\n",
    "\n",
    "\n",
    "# \"Text\"@\"Chapter\"@\"Book\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self supervised learning -> no labels needed for Language Modeling\n",
    "- target sentence, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 12 12:40:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:84:00.0 Off |                  N/A |\n",
      "| 24%   33C    P8    23W / 250W |  10962MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:85:00.0 Off |                  N/A |\n",
      "| 22%   34C    P8    19W / 250W |   9996MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |\n",
      "| 22%   28C    P8     6W / 250W |   3966MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 36%   58C    P2    90W / 250W |   7349MiB / 11264MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4238      C                                     161MiB |\n",
      "|    0   N/A  N/A      4491      C                                     161MiB |\n",
      "|    0   N/A  N/A      6550      C                                     161MiB |\n",
      "|    0   N/A  N/A      8554      C                                     161MiB |\n",
      "|    0   N/A  N/A      9540      C                                     161MiB |\n",
      "|    0   N/A  N/A     27839      C                                    9993MiB |\n",
      "|    0   N/A  N/A     32155      C                                     161MiB |\n",
      "|    1   N/A  N/A     25027      C                                    9993MiB |\n",
      "|    2   N/A  N/A     14712      C                                    3963MiB |\n",
      "|    3   N/A  N/A      7609      C                                    3283MiB |\n",
      "|    3   N/A  N/A     25347      C                                    4063MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Importing Pandas package\n",
    "\n",
    "Bookfile=[] #Empty \"Book\" list - Prepare for loop\n",
    "\n",
    "# Loops through importing 7 HP text files - Book 1 creates table, Books 2-7 append to Book 1 table\n",
    "for i in range(1,8): \n",
    "    Bookfile.append('HPBook'+str(i)+'.txt')\n",
    "    FileLoc = \"books/{}\".format(Bookfile[i-1])\n",
    "    if i == 1:\n",
    "        hp_df = pd.read_csv(FileLoc, sep=\"@\")\n",
    "    else:\n",
    "        hp_df2 = pd.read_csv(FileLoc, sep=\"@\")\n",
    "        hp_df = pd.concat([hp_df, hp_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE VANISHING GLASS  Nearly ten years had pass...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THE LETTERS FROM NO ONE  The escape of the Bra...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE KEEPER OF THE KEYS  BOOM. They knocked aga...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIAGON ALLEY  Harry woke early the next mornin...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Harry remained kneeling at Snape's side, simpl...</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Finally, the truth. Lying with his face presse...</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>He lay facedown, listening to the silence. He ...</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>He was flying facedown on the ground again. Th...</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Autumn seemed to arrive suddenly that year. Th...</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Chapter  Book\n",
       "0   THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...        1     1\n",
       "1   THE VANISHING GLASS  Nearly ten years had pass...        2     1\n",
       "2   THE LETTERS FROM NO ONE  The escape of the Bra...        3     1\n",
       "3   THE KEEPER OF THE KEYS  BOOM. They knocked aga...        4     1\n",
       "4   DIAGON ALLEY  Harry woke early the next mornin...        5     1\n",
       "..                                                ...      ...   ...\n",
       "32  Harry remained kneeling at Snape's side, simpl...       33     7\n",
       "33  Finally, the truth. Lying with his face presse...       34     7\n",
       "34  He lay facedown, listening to the silence. He ...       35     7\n",
       "35  He was flying facedown on the ground again. Th...       36     7\n",
       "36  Autumn seemed to arrive suddenly that year. Th...       37     7\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE VANISHING GLASS  Nearly ten years had pass...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THE LETTERS FROM NO ONE  The escape of the Bra...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE KEEPER OF THE KEYS  BOOM. They knocked aga...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIAGON ALLEY  Harry woke early the next mornin...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Chapter  Book\n",
       "0  THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...        1     1\n",
       "1  THE VANISHING GLASS  Nearly ten years had pass...        2     1\n",
       "2  THE LETTERS FROM NO ONE  The escape of the Bra...        3     1\n",
       "3  THE KEEPER OF THE KEYS  BOOM. They knocked aga...        4     1\n",
       "4  DIAGON ALLEY  Harry woke early the next mornin...        5     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...\n",
       "1     THE VANISHING GLASS  Nearly ten years had pass...\n",
       "2     THE LETTERS FROM NO ONE  The escape of the Bra...\n",
       "3     THE KEEPER OF THE KEYS  BOOM. They knocked aga...\n",
       "4     DIAGON ALLEY  Harry woke early the next mornin...\n",
       "                            ...                        \n",
       "32    Harry remained kneeling at Snape's side, simpl...\n",
       "33    Finally, the truth. Lying with his face presse...\n",
       "34    He lay facedown, listening to the silence. He ...\n",
       "35    He was flying facedown on the ground again. Th...\n",
       "36    Autumn seemed to arrive suddenly that year. Th...\n",
       "Name: Text, Length: 200, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_df.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For future Reference -> REGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Fran -> ^ start\n",
    "\n",
    "$gun -> Sagun -> end\n",
    "\n",
    "\\d -> numerical digit\n",
    "\n",
    "\\s -> whitespace\n",
    "\n",
    "\\w -> any letter lower or uppercase,anydigit or underscore\n",
    "\n",
    "{7} -> number of occurences\n",
    "\n",
    "[aeiou]{2} -> 2 times aeiou occcurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =[]\n",
    "for x in hp_df.Text[0]:\n",
    "    text.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\n",
      "Did he mind?\n",
      "Adam Jones Jr. thinks he didn't.\n",
      "In any case, this isn't true...\n",
      "Well, with a probability of .9 it isn't.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"\"\"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\"\"\n",
    "m = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', s)\n",
    "for i in m:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_books=\"\"\n",
    "for x,y in enumerate(hp_df.Text):\n",
    "    all_books = all_books+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38831457/regex-splitting-strings-at-full-stops-unless-its-part-of-an-honorific#:~:text=l%20%3D%20set(%5B%27Mr.%27%2C%20%27Mrs.%27%2C%20%27Ms.%27%2C%20%27Dr.%27%2C%20%27Prof.%27%2C%20%27Rev.%27%2C%20%27Capt.%27%2C%20%27Lt.%2DCol.%27%2C%20%0A%27Col.%27%2C%20%27Lt.%2DCmdr.%27%2C%20%27The%20Hon.%27%2C%20%27Cmdr.%27%2C%20%27Flt.%20Lt.%27%2C%20%27Brgdr.%27%2C%20%27Wng.%20Cmdr.%27%2C%20%0A%27Group%20Capt.%27%20%2C%27Rt.%27%2C%20%27Maj.%2DGen.%27%2C%20%27Rear%20Admrl.%27%2C%20%27Esq.%27%2C%20%27Mx%27%2C%20%27Adv%27%2C%20%27Jr.%27%5D%20)\n",
    "\n",
    "l = set(['Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.', 'Rev.', 'Capt.', 'Lt.-Col.', \n",
    "'Col.', 'Lt.-Cmdr.', 'The Hon.', 'Cmdr.', 'Flt. Lt.', 'Brgdr.', 'Wng. Cmdr.', \n",
    "'Group Capt.' ,'Rt.', 'Maj.-Gen.', 'Rear Admrl.', 'Esq.', 'Mx', 'Adv', 'Jr.'])\n",
    "\n",
    "final_text = (''.join([w + '\\n' if '.' in w and w not in l else w + ' ' for w in all_books.split(' ')]).rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_df_fin = pd.DataFrame(columns=['sentence'])\n",
    "\n",
    "data_df = pd.DataFrame([sentence for sentence in final_text.split('\\n') if sentence],\n",
    "                       columns=['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75264, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They were the last people you'd expect to be i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Dursley was the director of a firm called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He was a big, beefy man with hardly any neck, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs. Dursley was thin and blonde and had nearl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...\n",
       "1  They were the last people you'd expect to be i...\n",
       "2   Mr. Dursley was the director of a firm called...\n",
       "3  He was a big, beefy man with hardly any neck, ...\n",
       "4  Mrs. Dursley was thin and blonde and had nearl..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = data_df.shape[0]\n",
    "train_idx = int(70/100 * total_rows)\n",
    "val_idx = int(20/100 * total_rows)\n",
    "test_idx = int(10/100 * total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52684, 15052, 7526)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx,val_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_df[0:train_idx]\n",
    "val_dataset = data_df[train_idx:train_idx+val_idx]\n",
    "test_dataset = data_df[train_idx+val_idx:train_idx+val_idx+test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52684, 1), (15052, 1), (7526, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape,val_dataset.shape,test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# dataset = Dataset.from_pandas(hp_df)\n",
    "train_dataset =Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "test_dataset =Dataset.from_pandas(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentences'],\n",
       "    num_rows: 52684\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['sentences'])}  \n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_data, remove_columns=['sentences'], fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_data, remove_columns=['sentences'], fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_data, remove_columns=['sentences'], fn_kwargs={'tokenizer': tokenizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 52684\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10117\n",
      "['<unk>', '<eos>', '.', ',', \"'\", 'the', 'to', 'and', 'of', 'a', 'he', '\\\\', 'harry', 'was', 'said', 's', 'his', 'it', 'you', 'in', 'i', '?', 'had', 'that', 'at', 't', 'on', '!', 'as', 'him', 'they', 'with', 'ron', '-', 'for', 'her', 'but', 'hermione', 'she', 'up', 'out', 'not', 'be', 'were', 'all', 'them', 'from', 'what', 'have', 'we']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_dataset['tokens'], \n",
    "min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:50])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = get_data(tokenized_train_dataset, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = get_data(tokenized_val_dataset, vocab, batch_size)\n",
    "test_data  = get_data(tokenized_train_dataset, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7718])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "# batch size, number of batches\n",
    "# 16k of 128 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_first -> If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NER -> use output\n",
    "- POS tagging -> output\n",
    "- Sentiment analysis -> mean of all hidden layers-  torch.mean output in 1\n",
    "                     -> can use both hidden and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):   \n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        #create embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "        #batch is \n",
    "        self.LSTM = nn.LSTM(emb_dim,hid_dim,num_layers=num_layers, dropout=dropout_rate,\n",
    "                           batch_first=True)\n",
    "        self.dropout= nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(hid_dim,vocab_size)\n",
    "        \n",
    "    def init_hidden(self,batch_size, devide):\n",
    "        #this function will in the beginning of the epoch\n",
    "        hidden = torch.zeros(self.num_layers,batch_size,self.hid_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers,batch_size,self.hid_dim).to(device)\n",
    "        \n",
    "        #return as tuple\n",
    "        return hidden,cell\n",
    "    \n",
    "    def detach_hidden(self,hidden):\n",
    "        hidden, cell = hidden\n",
    "        #detaches from cpu/gpu\n",
    "        #remove from gradients cause it doens't make sense to use gradients for hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src ; batchsize, seq_length\n",
    "        \n",
    "        #embed\n",
    "        embed = self.embedding(src)\n",
    "        #batch_size,seq_len,emb_dim\n",
    "        \n",
    "        #now send this to the LSTM\n",
    "        #hidden has -> hidden state, cell state\n",
    "        #we want to put hidden here, cause we want to reset hidden\n",
    "        #we always reset cause we don't learn hidden\n",
    "        output,hidden = self.LSTM(embed,hidden)\n",
    "        #output: batch_size,seq_len,hid_dim\n",
    "        #hidden: num_layer,batch_size,hid_dim\n",
    "        \n",
    "        output  = self.dropout(output)\n",
    "        \n",
    "        prediction = self.fc(output)\n",
    "        #we use output for prediction\n",
    "        #batchsize, seq_len, vocab_size\n",
    "        \n",
    "        return prediction,hidden\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size, emb_dim, hid_dim, num_layers, dro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,523,333 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #this data is fromget_data\n",
    "    #\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model\n",
    "#  data = raw data\n",
    "# optimizer = Adam\n",
    "# criterion = Loss function\n",
    "# batch size = size of the batch\n",
    "# sequence length = batch ma bhako \n",
    "# clip = precents gradient explosion\n",
    "# device - the device\n",
    "\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 200.350\n",
      "\tValid Perplexity: 121.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 87.454\n",
      "\tValid Perplexity: 93.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 68.660\n",
      "\tValid Perplexity: 82.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 59.073\n",
      "\tValid Perplexity: 76.360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 52.827\n",
      "\tValid Perplexity: 72.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 48.248\n",
      "\tValid Perplexity: 69.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 44.578\n",
      "\tValid Perplexity: 68.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.726\n",
      "\tValid Perplexity: 67.567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.173\n",
      "\tValid Perplexity: 67.440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 36.975\n",
      "\tValid Perplexity: 66.850\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "seq_len  = 50\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-harry_potter_lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 26.165\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-harry_potter_lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- temperature \n",
    "- softmax -> argmax -> temp is 1\n",
    "- higher probability gets more chance in softmax hill jasto graph huncha softmax ko\n",
    "- temperature is dividing this probability\n",
    "- end/seq_len bhetesi end huncha sentence generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "hogwarts was still very difficult .\n",
      "\n",
      "0.7\n",
      "hogwarts discord had been attacked by the fat lady , and because he was scared of the school .\n",
      "\n",
      "0.75\n",
      "hogwarts discord had been attacked by the fat lady , and because he was scared of the dangers of fighting himself in the distance , harry felt it if it was\n",
      "\n",
      "0.8\n",
      "hogwarts discord had been attacked by the fat lady , and because none of the dursleys were spending more fighting the confidence of jupiter ' s memories , if it was\n",
      "\n",
      "1.0\n",
      "hogwarts discord had patched most of the fat lady in a because in hearing the dursleys ' statute of fighting investigate , but in two years ago , if it was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'hogwarts'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/ErikaJacobs/Harry-Potter-Text-Mining/blob/master/Notebooks/HP_Text_Mining%20-%20Exploratory%20Data%20Analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
