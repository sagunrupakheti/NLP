# Transformers: State-of-the-Art Natural Language Processing

| Paper Background   |                       |
| -------------------| ----------------------|
| Authors            | Thomas Wolf et. al    |
| Year               | 2020                  |
| Journal            | EMNLP                 |
| Organization       | Hugging Face          |

-----------------------------------------------

|Introduction and Background|
|-|
|->Transformer is a type of neural network architecture for natural language processing tasks, introduced in 2017. It uses self-attention mechanisms to process input sequences, allowing it to effectively handle long-range dependencies and process large amounts of data.|
|What is new?|
|->Introduces the concept of attention, differentially weighing the significance of each part of the input data.|
|Where is it mainly used?|
|->Natural language processing tasks|
|Key Terms|
|->Attention, Self Attention, Multi-head Self Attention|
|Problems Solved by Transformers|
|-> Sequential processing,Long-range dependencies,Attention mechanism,Model parallelism|