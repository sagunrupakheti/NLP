{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 4: TorchText + biGRU + Attention + Masking + Padded + Teaching Forcing + BLEU\n",
    "\n",
    "In this notebook we will be adding a few improvements - packed padded sequences and masking.  Packed padded sequences are used to tell our RNN to skip over padding tokens in our encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements. Both of these techniques are commonly used in NLP. \n",
    "\n",
    "We will also look at how to use our model for inference, by giving it a sentence, seeing what it translates it as and seeing where exactly it pays attention to when translating each word.\n",
    "\n",
    "Finally, we'll use the BLEU metric to measure the quality of our translations.\n",
    "\n",
    "The figures are from https://github.com/bentrevett.\n",
    "\n",
    "**Note**: Skip to the model part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  6 10:52:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:84:00.0 Off |                  N/A |\n",
      "| 24%   31C    P8    23W / 250W |    969MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:85:00.0 Off |                  N/A |\n",
      "| 22%   33C    P8    18W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |\n",
      "| 22%   27C    P8     6W / 250W |   7716MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 22%   26C    P8     5W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4238      C                                     161MiB |\n",
      "|    0   N/A  N/A      4491      C                                     161MiB |\n",
      "|    0   N/A  N/A      6550      C                                     161MiB |\n",
      "|    0   N/A  N/A      8554      C                                     161MiB |\n",
      "|    0   N/A  N/A      9540      C                                     161MiB |\n",
      "|    0   N/A  N/A     32155      C                                     161MiB |\n",
      "|    2   N/A  N/A     18354      C                                    7713MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "with gzip.open(\"en-ne.txt.gz\", mode=\"rt\") as f:\n",
    "    file_content = f.read()\n",
    "    # print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\t \\n\n",
    "# file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = file_content.replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(StringIO(file_content),sep='\\t|\\n',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>York is a collegiate university and every stud...</td>\n",
       "      <td>न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40:8 All my enemies were whispering against me...</td>\n",
       "      <td>40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During the Presidency of Edmund J . James (190...</td>\n",
       "      <td>पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These sequences are intercepted by the ANSI.SY...</td>\n",
       "      <td>यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do all my brothers, sisters, disillusioned, an...</td>\n",
       "      <td>मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  York is a collegiate university and every stud...   \n",
       "1  40:8 All my enemies were whispering against me...   \n",
       "2  During the Presidency of Edmund J . James (190...   \n",
       "3  These sequences are intercepted by the ANSI.SY...   \n",
       "4  Do all my brothers, sisters, disillusioned, an...   \n",
       "\n",
       "                                                   1  \n",
       "0  न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...  \n",
       "1  40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...  \n",
       "2  पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...  \n",
       "3  यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...  \n",
       "4  मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184168"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92084, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# total of 92084 pairs\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.rename({0: 'eng', 1: 'nep'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>nep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>York is a collegiate university and every stud...</td>\n",
       "      <td>न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40:8 All my enemies were whispering against me...</td>\n",
       "      <td>40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During the Presidency of Edmund J . James (190...</td>\n",
       "      <td>पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These sequences are intercepted by the ANSI.SY...</td>\n",
       "      <td>यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do all my brothers, sisters, disillusioned, an...</td>\n",
       "      <td>मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng  \\\n",
       "0  York is a collegiate university and every stud...   \n",
       "1  40:8 All my enemies were whispering against me...   \n",
       "2  During the Presidency of Edmund J . James (190...   \n",
       "3  These sequences are intercepted by the ANSI.SY...   \n",
       "4  Do all my brothers, sisters, disillusioned, an...   \n",
       "\n",
       "                                                 nep  \n",
       "0  न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...  \n",
       "1  40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...  \n",
       "2  पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...  \n",
       "3  यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...  \n",
       "4  मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"nep\",\"eng\"]\n",
    "df2=df2.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nep</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...</td>\n",
       "      <td>York is a collegiate university and every stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...</td>\n",
       "      <td>40:8 All my enemies were whispering against me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...</td>\n",
       "      <td>During the Presidency of Edmund J . James (190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...</td>\n",
       "      <td>These sequences are intercepted by the ANSI.SY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...</td>\n",
       "      <td>Do all my brothers, sisters, disillusioned, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 nep  \\\n",
       "0  न्यूयोर्क एक collegiate विश्वविद्यालय छ र हरेक...   \n",
       "1  40:8 मेरो सबै शत्रुलाई मेरो विरुद्धमा कानेखुसी...   \n",
       "2  पुस्तकालय, जसमा स्कूल संग खोलियो 1868, सुरु 1,...   \n",
       "3  यी दृश्यहरु पीसी गरेको एक नम्बर मा CONFIG.SYS ...   \n",
       "4  मेरा सबै भाइहरूलाई के, बहिनीहरू, रनभुल्लमा, र ...   \n",
       "\n",
       "                                                 eng  \n",
       "0  York is a collegiate university and every stud...  \n",
       "1  40:8 All my enemies were whispering against me...  \n",
       "2  During the Presidency of Edmund J . James (190...  \n",
       "3  These sequences are intercepted by the ANSI.SY...  \n",
       "4  Do all my brothers, sisters, disillusioned, an...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment this if you are not using our department puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "# from torchtext.datasets import Multi30k\n",
    "\n",
    "# SRC_LANGUAGE = 'en'\n",
    "# TRG_LANGUAGE = 'de'\n",
    "\n",
    "# train = Multi30k(split=('train'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at one example of train\n",
    "# sample = next(iter(_lo)\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = len(list(iter(train)))\n",
    "# train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df2, test_size=0.2,random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73667, 18417)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data =  train_test_split(train, test_size=0.3,random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51566, 22101, 18417)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(val_data),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nep</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>समय प्रति समय सेवाको रूपरेखा बदलिइरहेको छ र बद...</td>\n",
       "      <td>You just have to know how to use it. You need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91116</th>\n",
       "      <td>बिरामी anointing को अभ्यास प्रेषितहरूको जस्तै ...</td>\n",
       "      <td>The practice of anointing the sick was an offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19345</th>\n",
       "      <td>अमेरिकामा पारस खड्का सहित ८ नेपाली क्रिकेटर Wi...</td>\n",
       "      <td>Nepal Rhinos face MSM England in 2X-Cricket Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72367</th>\n",
       "      <td>115 पछिल्लो पिउनुपर्छ पानी सेवा गर्दै एक विषयम...</td>\n",
       "      <td>ON THE SUBJECT OF 115 THE ONE SERVING WATER SH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55886</th>\n",
       "      <td>हामीलाई सम्पर्क गर्नुहोस: सम्पर्क[मा]binaryopt...</td>\n",
       "      <td>Contact us: contact [at] binaryoptionsindicato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33234</th>\n",
       "      <td>10 पन्डुलिपी-कृष्ण धराबाशी | Listen and Download</td>\n",
       "      <td>Muktak Madhurima - 2069-10-11 | Listen And Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71296</th>\n",
       "      <td>a92a40g69b पाकिस्तान म हुं पुरुष, 31, खोज्दै म...</td>\n",
       "      <td>a92a40g69b Pakistan I am a man, 31, seeking a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30894</th>\n",
       "      <td>केचिनी एम ए, रूट डीई, रचउनव जेआर, र गेल्ब पी ए...</td>\n",
       "      <td>Cecchini MA, Root DE, Rachunow JR, and Gelb PM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77870</th>\n",
       "      <td>जाडो बिदा छोराको दिनमा 4 मार्च. 18:00 यो Allbe...</td>\n",
       "      <td>Before winter's day 4 March at. 18:00 it Allbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74015</th>\n",
       "      <td>निजी जेट चार्टर सेवा डेलावेयर – निजी जेट एयर च...</td>\n",
       "      <td>Private jet charter service Delaware – Private...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51566 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     nep  \\\n",
       "4695   समय प्रति समय सेवाको रूपरेखा बदलिइरहेको छ र बद...   \n",
       "91116  बिरामी anointing को अभ्यास प्रेषितहरूको जस्तै ...   \n",
       "19345  अमेरिकामा पारस खड्का सहित ८ नेपाली क्रिकेटर Wi...   \n",
       "72367  115 पछिल्लो पिउनुपर्छ पानी सेवा गर्दै एक विषयम...   \n",
       "55886  हामीलाई सम्पर्क गर्नुहोस: सम्पर्क[मा]binaryopt...   \n",
       "...                                                  ...   \n",
       "33234   10 पन्डुलिपी-कृष्ण धराबाशी | Listen and Download   \n",
       "71296  a92a40g69b पाकिस्तान म हुं पुरुष, 31, खोज्दै म...   \n",
       "30894  केचिनी एम ए, रूट डीई, रचउनव जेआर, र गेल्ब पी ए...   \n",
       "77870  जाडो बिदा छोराको दिनमा 4 मार्च. 18:00 यो Allbe...   \n",
       "74015  निजी जेट चार्टर सेवा डेलावेयर – निजी जेट एयर च...   \n",
       "\n",
       "                                                     eng  \n",
       "4695   You just have to know how to use it. You need ...  \n",
       "91116  The practice of anointing the sick was an offi...  \n",
       "19345  Nepal Rhinos face MSM England in 2X-Cricket Cu...  \n",
       "72367  ON THE SUBJECT OF 115 THE ONE SERVING WATER SH...  \n",
       "55886  Contact us: contact [at] binaryoptionsindicato...  \n",
       "...                                                  ...  \n",
       "33234  Muktak Madhurima - 2069-10-11 | Listen And Dow...  \n",
       "71296  a92a40g69b Pakistan I am a man, 31, seeking a ...  \n",
       "30894  Cecchini MA, Root DE, Rachunow JR, and Gelb PM...  \n",
       "77870  Before winter's day 4 March at. 18:00 it Allbe...  \n",
       "74015  Private jet charter service Delaware – Private...  \n",
       "\n",
       "[51566 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -inltk -> for indian subcontinent languages\n",
    "# https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepali language tokenizer -> nepalitokenizer\n",
    "# pip install nepalitokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from nepalitokenizer import NepaliTokenizer\n",
    "\n",
    "SRC_LANGUAGE= 'nep'\n",
    "TRG_LANGUAGE= 'eng'\n",
    "\n",
    "tokenize = NepaliTokenizer()\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = tokenize.tokenizer\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तिमीलाई', 'माया', 'प्रिय']\n"
     ]
    }
   ],
   "source": [
    "# let's try tokenizing nepali language\n",
    "\n",
    "tokenize = NepaliTokenizer()\n",
    "print(tokenize.tokenizer('म तिमीलाई धेरै माया गर्छु मेरो प्रिय'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stop words also get removed\n",
    "- Stop words are in this sentence -> म- I , धेरै - lots, गर्छु - do, मेरो - mine\n",
    "\n",
    "https://github.com/xettrisomeman/tokenize/blob/main/src/nepalitokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['तिमीलाई', 'माया', 'प्रिय']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform[SRC_LANGUAGE]('म तिमीलाई धेरै माया गर्छु मेरो प्रिय')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:  ['I', 'love', 'you', 'very', 'much', 'my', 'sweetheart']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "# print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE]('I love you very much my sweetheart'))\n",
    "\n",
    "# stop words do not get removed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(EOS_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object yield_tokens at 0x7f48ccc2d660>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yield_tokens(train_data, TRG_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab_from_iterator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making it iterable\n",
    "# Note: The function build_vocab_from_iterator builds a vocab using a iterator only\n",
    "# up until now, we have a dataframe so converting them to list\n",
    "# makes it iterable\n",
    "\n",
    "train_data = train_data.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_data, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[299, 13, 11, 0, 11]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1914, 262, 4, 2795]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[SRC_LANGUAGE](['बिरामी', 'कार्यालय', 'तपाईं','माया'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'निकट'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[3423]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48403"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.values.tolist()\n",
    "test_data = test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_train = next(iter(train_data))\n",
    "# sample_train\n",
    "\n",
    "# sample_val = next(iter(val_data))\n",
    "# sample_val\n",
    "\n",
    "# sample_test = next(iter(test_data))\n",
    "# sample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train=[]\n",
    "sample_val=[]\n",
    "sample_test= []\n",
    "for x,y in enumerate(train_data):\n",
    "    #x - index\n",
    "    if x == 5000:\n",
    "        break\n",
    "    sample_train.append(y)\n",
    "    \n",
    "for x,y in enumerate(val_data):\n",
    "    #x - index\n",
    "    if x == 4000:\n",
    "        break\n",
    "    sample_val.append(y)\n",
    "\n",
    "\n",
    "for x,y in enumerate(test_data):\n",
    "    #x - index\n",
    "    if x == 3000:\n",
    "        break\n",
    "    sample_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "sample_train_loader = DataLoader(sample_train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "sample_valid_loader = DataLoader(sample_val, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "sample_test_loader  = DataLoader(sample_test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nep, _, eng in sample_train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nep, _, eng in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nepali shape:  torch.Size([47, 8])\n",
      "English shape:  torch.Size([79, 8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Nepali shape: \", nep.shape)  # (seq len, batch_size)\n",
    "print(\"English shape: \", eng.shape)   # (seq len, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The changes here all within the `forward` method. It now accepts the lengths of the source sentences as well as the sentences themselves. \n",
    "\n",
    "After the source sentence (padded automatically within the iterator) has been embedded, we can then use `pack_padded_sequence` on it with the lengths of the sentences. Note that the tensor containing the lengths of the sequences must be a CPU tensor as of the latest version of PyTorch, which we explicitly do so with `to('cpu')`. `packed_embedded` will then be our packed padded sequence. This can be then fed to our RNN as normal which will return `packed_outputs`, a packed tensor containing all of the hidden states from the sequence, and `hidden` which is simply the final hidden state from our sequence. `hidden` is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final **non-padded element** in the sequence.\n",
    "\n",
    "We then unpack our `packed_outputs` using `pad_packed_sequence` which returns the `outputs` and the lengths of each, which we don't need. \n",
    "\n",
    "The first dimension of `outputs` is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)        \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * 2]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "The attention used here is additive attention which is defined by:\n",
    "\n",
    "$$e = v\\text{tanh}(W_hh + W_ss + b)$$\n",
    "\n",
    "Previously, we allowed this module to \"pay attention\" to padding tokens within the source sentence. However, using *masking*, we can force the attention to only be over non-padding elements.\n",
    "\n",
    "The `forward` method now takes a `mask` input. This is a `[batch size, source sentence length]` tensor that is 1 when the source sentence token is not a padding token, and 0 when it is a padding token. For example, if the source sentence is: `[\"hello\", \"how\", \"are\", \"you\", \"?\", `<pad>`, `<pad>`]`, then the mask would be `[1, 1, 1, 1, 1, 0, 0]`.\n",
    "\n",
    "We apply the mask after the attention has been calculated, but before it has been normalized by the `softmax` function. It is applied using `masked_fill`. This fills the tensor at each element where the first argument (`mask == 0`) is true, with the value given by the second argument (`-1e10`). In other words, it will take the un-normalized attention values, and change the attention values over padded elements to be `-1e10`. As these numbers will be miniscule compared to the other values they will become zero when passed through the `softmax` layer, ensuring no attention is payed to padding tokens in the source sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs\n",
    "                \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch size, src len, hid dim]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "        #energy = [batch size, src len, hid dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        #use masked_fill_ if you want in-place\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the masked_fill work, just so you have faith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[           9, -10000000000,            7,            2, -10000000000,\n",
      "         -10000000000],\n",
      "        [          99, -10000000000, -10000000000,            0,            8,\n",
      "                    9]])\n"
     ]
    }
   ],
   "source": [
    "#example of masked_fill\n",
    "#reall that 1 is pad_idx\n",
    "x = torch.tensor([ [9, 1, 7, 2, 1, 1], [99, 1, 1, 0, 8, 9] ])\n",
    "\n",
    "mask = (x == PAD_IDX)\n",
    "\n",
    "x.masked_fill_(mask, -1e10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted = [batch size, 1, hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #this also means that output == hidden\n",
    "        # assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so it's the same shape as attention\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attentiont outputs from decoder\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input_ = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output    = [batch size, output dim]\n",
    "            #hidden    = [batch size, hid dim]\n",
    "            #attention = [batch size, src len]\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #place attentions in a tensor holding attention for each token\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training\n",
    "\n",
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(45155, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(48403, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=48403, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim)\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11559680\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "12391168\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "86738176\n",
      " 48403\n",
      "______\n",
      "117171219\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is very similar to part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  6 10:56:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:84:00.0 Off |                  N/A |\n",
      "| 24%   31C    P8    23W / 250W |    969MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:85:00.0 Off |                  N/A |\n",
      "| 22%   33C    P8    18W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |\n",
      "| 22%   27C    P8     6W / 250W |   7716MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 22%   36C    P2    51W / 250W |   1268MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4238      C                                     161MiB |\n",
      "|    0   N/A  N/A      4491      C                                     161MiB |\n",
      "|    0   N/A  N/A      6550      C                                     161MiB |\n",
      "|    0   N/A  N/A      8554      C                                     161MiB |\n",
      "|    0   N/A  N/A      9540      C                                     161MiB |\n",
      "|    0   N/A  N/A     32155      C                                     161MiB |\n",
      "|    2   N/A  N/A     18354      C                                    7713MiB |\n",
      "|    3   N/A  N/A      8190      C                                    1265MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        \n",
    "        #here?\n",
    "        src = src.to(device).long()\n",
    "        trg = trg.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print('src',type(src))\n",
    "        # print('trg', type(trg))\n",
    "        # print('src_length' , type(src_length))\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device).long()\n",
    "            trg = trg.to(device).long()\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_loader_length = len(list(iter(sample_train_loader)))\n",
    "sample_val_loader_length   = len(list(iter(sample_valid_loader)))\n",
    "sample_test_loader_length  = len(list(iter(sample_test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(sample_val_loader_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Reduced dataset -->\n",
    "Epoch: 01 | Time: 1m 16s\n",
    "\tTrain Loss: 6.007 | Train PPL: 406.258\n",
    "\t Val. Loss: 6.296 |  Val. PPL: 542.175\n",
    "Epoch: 02 | Time: 1m 16s\n",
    "\tTrain Loss: 4.500 | Train PPL:  90.004\n",
    "\t Val. Loss: 6.734 |  Val. PPL: 840.327\n",
    "Epoch: 03 | Time: 1m 15s\n",
    "\tTrain Loss: 3.416 | Train PPL:  30.442\n",
    "\t Val. Loss: 6.951 |  Val. PPL: 1044.226\n",
    "Epoch: 04 | Time: 1m 16s\n",
    "\tTrain Loss: 2.909 | Train PPL:  18.347\n",
    "\t Val. Loss: 7.089 |  Val. PPL: 1198.446\n",
    "Epoch: 05 | Time: 1m 16s\n",
    "\tTrain Loss: 2.618 | Train PPL:  13.709\n",
    "\t Val. Loss: 7.132 |  Val. PPL: 1251.044\n",
    "Epoch: 06 | Time: 1m 15s\n",
    "\tTrain Loss: 2.298 | Train PPL:   9.954\n",
    "\t Val. Loss: 7.246 |  Val. PPL: 1402.935\n",
    "Epoch: 07 | Time: 1m 16s\n",
    "\tTrain Loss: 2.091 | Train PPL:   8.096\n",
    "\t Val. Loss: 7.366 |  Val. PPL: 1581.577\n",
    "Epoch: 08 | Time: 1m 14s\n",
    "\tTrain Loss: 1.850 | Train PPL:   6.358\n",
    "\t Val. Loss: 7.556 |  Val. PPL: 1912.729\n",
    "Epoch: 09 | Time: 1m 17s\n",
    "\tTrain Loss: 1.621 | Train PPL:   5.060\n",
    "\t Val. Loss: 7.757 |  Val. PPL: 2337.585\n",
    "Epoch: 10 | Time: 1m 16s\n",
    "\tTrain Loss: 1.464 | Train PPL:   4.325\n",
    "\t Val. Loss: 7.814 |  Val. PPL: 2475.264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 2 µs, total: 11 µs\n",
      "Wall time: 19.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1678100274.9830961\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678100828.7265089\n",
      "Epoch: 01 | Time: 9m 13s\n",
      "\tTrain Loss: 7.502 | Train PPL: 1811.501\n",
      "\t Val. Loss: 7.078 |  Val. PPL: 1186.006\n",
      "1678100829.7035546\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678101386.0598128\n",
      "Epoch: 02 | Time: 9m 16s\n",
      "\tTrain Loss: 6.644 | Train PPL: 767.819\n",
      "\t Val. Loss: 6.971 |  Val. PPL: 1065.460\n",
      "1678101389.155271\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678101943.5346925\n",
      "Epoch: 03 | Time: 9m 14s\n",
      "\tTrain Loss: 6.075 | Train PPL: 434.639\n",
      "\t Val. Loss: 6.954 |  Val. PPL: 1047.699\n",
      "1678101947.0408313\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678102510.1765926\n",
      "Epoch: 04 | Time: 9m 23s\n",
      "\tTrain Loss: 5.514 | Train PPL: 248.151\n",
      "\t Val. Loss: 7.037 |  Val. PPL: 1138.340\n",
      "1678102510.176953\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678103066.9026668\n",
      "Epoch: 05 | Time: 9m 16s\n",
      "\tTrain Loss: 4.896 | Train PPL: 133.751\n",
      "\t Val. Loss: 7.242 |  Val. PPL: 1396.403\n",
      "1678103066.9030297\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678103618.4913557\n",
      "Epoch: 06 | Time: 9m 11s\n",
      "\tTrain Loss: 4.272 | Train PPL:  71.664\n",
      "\t Val. Loss: 7.508 |  Val. PPL: 1822.396\n",
      "1678103618.4917462\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678104179.0573845\n",
      "Epoch: 07 | Time: 9m 20s\n",
      "\tTrain Loss: 3.884 | Train PPL:  48.613\n",
      "\t Val. Loss: 7.702 |  Val. PPL: 2212.448\n",
      "1678104179.0577679\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678104737.756892\n",
      "Epoch: 08 | Time: 9m 18s\n",
      "\tTrain Loss: 3.611 | Train PPL:  36.990\n",
      "\t Val. Loss: 7.793 |  Val. PPL: 2423.100\n",
      "1678104737.7572596\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678105292.764714\n",
      "Epoch: 09 | Time: 9m 15s\n",
      "\tTrain Loss: 3.369 | Train PPL:  29.035\n",
      "\t Val. Loss: 7.957 |  Val. PPL: 2856.509\n",
      "1678105292.76508\n",
      "Inside training now.....\n",
      "Inside evaluation now.....\n",
      "1678105848.1504714\n",
      "Epoch: 10 | Time: 9m 15s\n",
      "\tTrain Loss: 3.161 | Train PPL:  23.596\n",
      "\t Val. Loss: 8.037 |  Val. PPL: 3092.030\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(start_time)\n",
    "    print('Inside training now.....')\n",
    "    train_loss = train(model, sample_train_loader, optimizer, criterion, clip, sample_train_loader_length)\n",
    "    print('Inside evaluation now.....')\n",
    "    valid_loss = evaluate(model, sample_valid_loader, criterion, sample_val_loader_length)\n",
    "    \n",
    "    # train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    # valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(end_time)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 42.834 | Test PPL: 4005338966564531200.000 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, sample_test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(45155, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(48403, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=48403, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_text_src = 'म तिमीलाई धेरै माया गर्छु मेरो प्रिय'\n",
    "random_text_trg = 'I love you very much'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  807, 2795, 1978,    3], device='cuda:3')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](random_text_src).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  28, 447,  23, 168, 259,   3], device='cuda:3')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](random_text_trg).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([7, 1]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, text_length, trg_text, 0) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1, 48403])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #trg_len, batch_size, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 48403])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 48403])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output[0]),type(output_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10116,  8603,  8603,  8603, 39567, 39521], device='cuda:3')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 44,  5,  5,  7,  5], device='cuda:3')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "And\n",
      "the\n",
      "the\n",
      "of\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for token in output.argmax(1):\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "And\n",
      "the\n",
      "the\n",
      "of\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1, 5])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'तिमीलाई', 'माया', 'प्रिय', '<eos>']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](random_text_src) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', '<unk>', 'And', 'the', 'the', 'of', 'the']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "font_prop = FontProperties(fname='akshar.ttf', size=18)\n",
    "\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45,fontproperties=font_prop)\n",
    "        \n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ax.set_fontproperties` not found.\n"
     ]
    }
   ],
   "source": [
    "ax.set_fontproperties?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', '<unk>', 'And', 'the', 'the', 'of', 'the']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20765/3653734894.py:21: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45,fontproperties=font_prop)\n",
      "/tmp/ipykernel_20765/3653734894.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice!  We get the similar perplexity but with much faster computations, thanks to packed sequences and masking.   We also can see attention by our eyes, so we should believe it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
